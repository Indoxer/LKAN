script: lkan.scripts.train
model: lkan.models.MLP
model_params:
  layers_dims: [2,5,1]
  device: ${trainer_params.device}
trainer: lkan.trainers.BasicMLPTrainer
trainer_params:
    lr: 0.001
    lr_step: null
    clip_grad_norm: 0.5
    accumulate_grad_batches: 1
    device: cuda
lr_scheduler: null
lr_scheduler_params: null
train_params:
  max_epochs: 10
  max_steps: 10000
  validation_every_n_steps: 100
  save_every_n_steps: 2500
datamodule: lkan.datamodule.TestDataModule
datamodule_params:
  batch_size: 16
  split_ratio: 0.8
save_dir: ./.experiments/${name}/${version}
name: basic_mlp
version: v1